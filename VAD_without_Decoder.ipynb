{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqIU-POExabQ"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-style: italic;\n",
    "        font-family: Roboto;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od4z7Mnqyx7S"
   },
   "source": [
    "<style>\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "## Mount drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28941,
     "status": "ok",
     "timestamp": 1732885910125,
     "user": {
      "displayName": "Phú Quốc Đinh",
      "userId": "18201426563846769527"
     },
     "user_tz": -420
    },
    "id": "tNR-qy8UfcYR",
    "outputId": "8c71c427-34b6-4333-8fb4-53c04aa26a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOXamkohGPCm"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "## Speech Folder Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTu7hPWUFwW_"
   },
   "outputs": [],
   "source": [
    "snr = \"0\"\n",
    "snr_str = str(snr).replace(\"-\", \"M\")\n",
    "snr_noise = str(snr).replace(\"MIX\", \"0\")\n",
    "snr_noise_str = snr_noise.replace(\"-\", \"M\")\n",
    "folder_name = f\"TIMIT_{snr_str}_FINAL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ix38jkey3bC"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "## Unzip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21353,
     "status": "ok",
     "timestamp": 1732889291730,
     "user": {
      "displayName": "Phú Quốc Đinh",
      "userId": "18201426563846769527"
     },
     "user_tz": -420
    },
    "id": "WfWfiNMHMC0p",
    "outputId": "e0b50032-a556-48ce-f891-f8b70e922730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully unzipped /content/drive/MyDrive/TDTU/voice-processing/TIMIT_0_FINAL.zip to /content/\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "\n",
    "zip_file_path =\\\n",
    " f\"/content/drive/MyDrive/TDTU/voice-processing/{folder_name}.zip\"\n",
    "extract_path = '/content/'\n",
    "\n",
    "\n",
    "try:\n",
    "  with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "  print(f\"Successfully unzipped {zip_file_path} to {extract_path}\")\n",
    "except FileNotFoundError:\n",
    "  print(f\"Error: File not found at {zip_file_path}\")\n",
    "except zipfile.BadZipFile:\n",
    "  print(f\"Error: Invalid zip file at {zip_file_path}\")\n",
    "except Exception as e:\n",
    "  print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTd58lNxnOSh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import pickle\n",
    "import IPython.display as ipd\n",
    "import time\n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "from re import I\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.metrics import auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6T81O_pHzQaK"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "## Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1732889291730,
     "user": {
      "displayName": "Phú Quốc Đinh",
      "userId": "18201426563846769527"
     },
     "user_tz": -420
    },
    "id": "C7vxSdN3Slxn",
    "outputId": "40f20753-135c-4423-9c98-8e97efc8f760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using {} device\".format(device))\n",
    "# %% The variables in this cell can be customised\n",
    "learning_rate = 100e-4 # Learning rate for the EB, FB and DB layers\n",
    "learning_rate_DN = 1e-5 # Learning rate for the discriminative network\n",
    "LR_factor = 0.7 # The factor with which to decrease the learning rate after each epoch\n",
    "l2_weight = 1e-4\n",
    "\n",
    "training_epochs = 10 # The number of epochs during training\n",
    "concatenates = 10 # The number of files to concatenate\n",
    "training_batch_size = 3  # The number of forward steps per backward step. Multiplied with \"concatenates\" this is the mini-batch size\n",
    "testing_batch_size = 200 # The number of files in the testing split. Validation split is half of this\n",
    "AN_weight = 0.1 # The scalar referred to as \"alpha\" in the paper\n",
    "output_folder = \"/content\" # The name of the folder in which to store the results and models\n",
    "\n",
    "\"\"\"Kernel sizes\"\"\"\n",
    "k_EB1 = 55\n",
    "k_EB2 = 160\n",
    "k_EB3 = 160\n",
    "k_EB4 = 160\n",
    "\n",
    "k_FB = 160\n",
    "\n",
    "k_DB1 = 0\n",
    "k_DB2 = 0\n",
    "k_DB3 = 0\n",
    "\n",
    "k_DN1 = 55\n",
    "k_DN2 = 15\n",
    "k_DN3 = 5\n",
    "\n",
    "# %%\n",
    "training = 0 # Flag denoting whether the model is being trained or tested\n",
    "validation = 0 # Flag denoting whether to use the testing or validation split\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_primary = nn.BCELoss()\n",
    "loss_secondary = nn.CrossEntropyLoss()\n",
    "\n",
    "training_results_big = {\n",
    "     \"training\" : [],\n",
    "     \"learning_rate\" : [],\n",
    "      \"epochs\":[],\n",
    "     \"loss_DB\":[],\n",
    "     \"loss_AN\":[],\n",
    "     \"test_TP\":[],\n",
    "     \"test_FP\":[],\n",
    "     \"test_TN\":[],\n",
    "     \"test_FN\":[],\n",
    "     \"time_passed\" : []\n",
    "\n",
    "    }\n",
    "\n",
    "training_results_AUC = {\n",
    "     \"TP\" : [],\n",
    "     \"FP\" : [],\n",
    "      \"TN\":[],\n",
    "     \"FN\":[],\n",
    "    }\n",
    "\n",
    "\n",
    "noises_buffer = []\n",
    "validation = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JY1LymYz1jq"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-style: italic;\n",
    "        font-family: Roboto;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxp0yNHs02Uq"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "## Get data from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K--GbtX1tFs2"
   },
   "outputs": [],
   "source": [
    "def get_paths_data(train:int=0) -> list[list[str, str]]:\n",
    "\n",
    "    \"\"\"\n",
    "     Parameters\n",
    "    ----------\n",
    "    train : int, optional\n",
    "        Flag to indicate whether to get paths for training data (1) or testing data (0). The default is 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list\n",
    "        The list of pairs of file paths. Each pair contains the path to the .wav file and the path to the corresponding label file.\n",
    "        <br>\n",
    "        The data list clipped to 2000 samples for faster processing.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Đường dẫn đến thư mục dữ liệu\n",
    "    base_path = f'/content/{folder_name}'\n",
    "    data_folder = 'TRAIN' if train else 'TEST'\n",
    "    data_folder_path = os.path.join(base_path, data_folder)\n",
    "\n",
    "    # Danh sách để lưu trữ các cặp đường dẫn\n",
    "    data = []\n",
    "\n",
    "    # Duyệt qua từng thư mục từ dr1 đến dr8\n",
    "    for dr in range(1, 9):\n",
    "      dr_folder = os.path.join(data_folder_path, f'DR{dr}')\n",
    "\n",
    "      # Tìm tất cả các thư mục con trong thư mục dr\n",
    "      subfolders = glob.glob(os.path.join(dr_folder, '*'))\n",
    "\n",
    "      for subfolder in subfolders:\n",
    "          # Tìm tất cả các file .wav trong thư mục con\n",
    "        wav_files = glob.glob(os.path.join(subfolder, '*.WAV'))\n",
    "        for wav_file in wav_files:\n",
    "\n",
    "          parts = wav_file.split(\"-\")\n",
    "\n",
    "          label_file = \"\";\n",
    "\n",
    "          if(len(parts) == 1):\n",
    "            label_file = wav_file\\\n",
    "            .replace(f'{folder_name}', f'{folder_name}/labels')[:-4]\n",
    "          else:\n",
    "\n",
    "            label_file = parts[0]\\\n",
    "            .replace(f'{folder_name}', f'{folder_name}/labels')\n",
    "\n",
    "          if os.path.exists(label_file):\n",
    "            data.append([wav_file, label_file])\n",
    "\n",
    "    return data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHNzIz3mv5Ui"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "## Get element from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqAow4EWz4gZ"
   },
   "outputs": [],
   "source": [
    "def get_element(idx: int, data: list[list[str, str]], train: int = 0) -> tuple[np.ndarray, np.ndarray, str, str]:\n",
    "    \"\"\"\n",
    "    Used in __getitem__ of the dataloader class. Takes the data and label paths and return the\n",
    "    data stored inside them\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idx : int\n",
    "        Index of the data to be unpackaged. Randomly generated by the dataloader class.\n",
    "    data : list\n",
    "        List containing both the path to the data file and its corresponding label file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_sequence : numpy array\n",
    "        The raw speech data in 16 bit format.\n",
    "    label_sequence : numpy array\n",
    "        The corresponding labels matching the speech data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data_path, label_path = data[idx]\n",
    "    SNR = \"-10\"\n",
    "\n",
    "    noise_type = 'CL'\n",
    "\n",
    "    parts = data_path.split(\"-\")\n",
    "\n",
    "    if(len(parts) != 1):\n",
    "      flag = 0\n",
    "      if(len(parts) == 4):\n",
    "        flag = 1\n",
    "\n",
    "      type_noise = parts[2 + flag].split(\".\")[0]\n",
    "      if(type_noise == 'BAB'):\n",
    "        noise_type = 'N1'\n",
    "      if(type_noise == 'FAC'):\n",
    "        noise_type = 'N2'\n",
    "\n",
    "\n",
    "    data_tensor = []\n",
    "    label_tensor = []\n",
    "\n",
    "    audio, sr = librosa.load(data_path, sr=None)\n",
    "\n",
    "    data_tensor.append(audio)\n",
    "    label_tensor.append(np.fromfile(label_path,sep=\"\\n\"))\n",
    "\n",
    "    data_tensor[-1] = data_tensor[-1][0:len(label_tensor[-1])*160]\n",
    "    x = np.zeros((1,1,sum(len(item) for item in data_tensor)))\n",
    "    y = np.zeros((sum(len(item) for item in label_tensor)))\n",
    "    x[0,0,:] = np.hstack(data_tensor)\n",
    "    y = np.hstack(label_tensor)\n",
    "\n",
    "    return x, y, noise_type, SNR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-m3tQlG1ut7"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "## Data Loader Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "### Train Data Loader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8687z2r8xXEp"
   },
   "outputs": [],
   "source": [
    "class TIMIT_train(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for the training data. Inherits from the torch.utils.data.Dataset class.\n",
    "    The __getitem__ method is used to load the data and labels from the TIMIT dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.data = get_paths_data(train=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return get_element(idx, self.data, train=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "### Test Data Loader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EW50ZA9p5WbG"
   },
   "outputs": [],
   "source": [
    "class TIMIT_test(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Dataset class for the testing data. Inherits from the torch.utils.data.Dataset class.\n",
    "    The __getitem__ method is used to load the data and labels from the TIMIT dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.data = get_paths_data(train=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Function for unpacking the speech and label files and save them into numpy arrays\n",
    "        \"\"\"\n",
    "        return get_element(idx, self.data, train=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHxl1_RWUy34"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "</style>\n",
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5Cw0G9Z2JUK"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-style: italic;\n",
    "        font-family: Roboto;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1.2em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "<span>\n",
    "<strong>The architecture of the model is following the image below.</strong>\n",
    "</span>\n",
    "<div style=\"text-align:center;margin:auto;\">\n",
    "    <img src=\"./imgs/image.png\" width=\"70%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YleCZKf0TFeI"
   },
   "outputs": [],
   "source": [
    "class VAD_model(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class for the Voice Activity Detection model. Inherits from the torch.nn.Module class.\n",
    "    Based on the model described in the paper \"Adversarial Multi-Task Deep Neural Networks for Robust Voice Activity Detection\".\n",
    "\n",
    "    The model consists of the following layers:\n",
    "    - 4 Encoder Blocks (EB)\n",
    "    - 1 Framing Block (FB)\n",
    "    - 3 Adversarial Network Blocks (AN)\n",
    "    And some dropout layers and batch normalization layers.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(VAD_model, self).__init__()\n",
    "        self.EB1 = nn.Conv1d(1,30, k_EB1,stride=1,padding='same')\n",
    "        torch.nn.init.kaiming_uniform_(self.EB1.weight, a=0.01, mode='fan_in',nonlinearity='leaky_relu')\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.01, inplace=False)\n",
    "        self.drop1 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.EB2 = nn.Conv1d(30,15, k_EB2,stride=1,padding='same')\n",
    "        torch.nn.init.kaiming_uniform_(self.EB2.weight, a=0.01, mode='fan_in',nonlinearity='leaky_relu')\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.01, inplace=False)\n",
    "        self.drop2 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.EB3 = nn.Conv1d(15,7, k_EB3,stride=1,padding='same')\n",
    "        torch.nn.init.kaiming_uniform_(self.EB3.weight, a=0.01, mode='fan_in',nonlinearity='leaky_relu')\n",
    "        self.relu3 = nn.LeakyReLU(negative_slope=0.01, inplace=False)\n",
    "        self.drop3 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.EB4 = nn.Conv1d(7,2, k_EB4,stride=1,padding='same')\n",
    "        torch.nn.init.kaiming_uniform_(self.EB4.weight, a=0.01, mode='fan_in',nonlinearity='leaky_relu')\n",
    "        self.relu4 = nn.LeakyReLU(negative_slope=0.01, inplace=False)\n",
    "        self.drop4 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.FB = nn.Conv1d(2,2, k_FB,stride=160, padding = 'valid')\n",
    "        torch.nn.init.xavier_normal_(self.FB.weight)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.drop5 = nn.Dropout(p=0.0)\n",
    "\n",
    "        self.sigmoid4 = nn.Sigmoid()\n",
    "\n",
    "        self.AN1 = nn.Conv1d(2,2, k_DN1,stride=1,padding='same', groups=1)\n",
    "        torch.nn.init.xavier_normal_(self.AN1.weight)\n",
    "        self.sigmoidAN1 = nn.Sigmoid()\n",
    "        self.drop6 = nn.Dropout(p=0.0)\n",
    "\n",
    "        self.AN2 = nn.Conv1d(2,2, k_DN2,stride=1,padding='same', groups=1)\n",
    "        torch.nn.init.xavier_normal_(self.AN2.weight)\n",
    "        self.sigmoidAN2 = nn.Sigmoid()\n",
    "        self.drop7 = nn.Dropout(p=0.0)\n",
    "\n",
    "        self.AN3 = nn.Conv1d(2,5, k_DN3,stride=1,padding='same', groups=1)\n",
    "        torch.nn.init.xavier_normal_(self.AN3.weight)\n",
    "        self.sigmoidAN3 = nn.Softmax(dim=1)\n",
    "        self.drop8 = nn.Dropout(p=0.0)\n",
    "\n",
    "        self.bnorm1 = nn.BatchNorm1d((30))\n",
    "        self.bnorm2 = nn.BatchNorm1d((15))\n",
    "        self.bnorm3 = nn.BatchNorm1d((7))\n",
    "        self.bnorm4 = nn.BatchNorm1d((2))\n",
    "\n",
    "    def forward(self, x : torch.Tensor, training : int = 0) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        The forward pass of the model. The input is passed through the layers of the model and the output is returned.\n",
    "        The output consists of two tensors: one for the voice activity detection and one for the adversarial network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor containing the speech data.\n",
    "        training : int, optional\n",
    "            Flag denoting whether the model is being trained or tested. The default is 0.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DB : torch.Tensor\n",
    "            The output tensor of decoder block of the model.\n",
    "        AN : torch.Tensor\n",
    "            The output tensor of the adversarial network block of the model.\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.to(device)\n",
    "        x = x/2**15\n",
    "\n",
    "        x = self.EB1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.bnorm1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.EB2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.bnorm2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.EB3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.bnorm3(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        x = self.EB4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.bnorm4(x)\n",
    "        x = self.drop4(x)\n",
    "\n",
    "        x = self.FB(x)\n",
    "        x = self.sigmoid1(x)\n",
    "        x = self.drop5(x)\n",
    "\n",
    "        DB = self.sigmoid4(x)\n",
    "\n",
    "        AN = self.AN1(x)\n",
    "        AN = self.sigmoidAN1(AN)\n",
    "        AN = self.drop6(AN)\n",
    "\n",
    "        AN = self.AN2(AN)\n",
    "        AN = self.sigmoidAN2(AN)\n",
    "        AN = self.drop7(AN)\n",
    "\n",
    "        AN = self.AN3(AN)\n",
    "        AN = self.sigmoidAN3(AN)\n",
    "\n",
    "        return DB, AN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O8iCjNt2wP7"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "</style>\n",
    "## Optimizers of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiM7iB8lTAUC"
   },
   "outputs": [],
   "source": [
    "VAD = VAD_model().to(device)\n",
    "\n",
    "\"\"\" Initialize the optimisers\"\"\"\n",
    "\n",
    "optimizer_EB1 = torch.optim.RMSprop(VAD.EB1.parameters(), lr=learning_rate)\n",
    "optimizer_EB2 = torch.optim.RMSprop(VAD.EB2.parameters(), lr=learning_rate)\n",
    "optimizer_EB3 = torch.optim.RMSprop(VAD.EB3.parameters(), lr=learning_rate)\n",
    "optimizer_EB4 = torch.optim.RMSprop(VAD.EB4.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer_FB = torch.optim.RMSprop(VAD.FB.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer_DN1 = torch.optim.RMSprop(VAD.AN1.parameters(), lr=learning_rate_DN)\n",
    "optimizer_DN2 = torch.optim.RMSprop(VAD.AN2.parameters(), lr=learning_rate_DN)\n",
    "optimizer_DN3 = torch.optim.RMSprop(VAD.AN3.parameters(), lr=learning_rate_DN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BrLDfvxYaG9"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "</style>\n",
    "# Training, Testing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAi1G8ZF3bCD"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-style: italic;\n",
    "        font-family: Roboto;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1.2em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "### Save model and result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wW0tr_e9rgcG"
   },
   "outputs": [],
   "source": [
    "def load_model(path : str) -> None:\n",
    "    \"\"\"\n",
    "    Function for loading a saved model\n",
    "    \"\"\"\n",
    "\n",
    "    global VAD\n",
    "\n",
    "    VAD.load_state_dict(torch.load(path), strict=False)\n",
    "\n",
    "    return VAD\n",
    "\n",
    "\n",
    "def save_results_AUC(res : dict, path : str) -> None:\n",
    "    \"\"\"\n",
    "    Function for saving the dictionary containing information on testing as a pickle file\n",
    "    \"\"\"\n",
    "    with open(path, \"wb\") as fp:\n",
    "        pickle.dump(res, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_results(path : str) -> None:\n",
    "    \"\"\"\n",
    "    Function for loading a saved pickle file\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as input_file:\n",
    "        return pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "### Update learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXvq-Ax5UHrY"
   },
   "outputs": [],
   "source": [
    "def update_learning_rates() -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Updates the learning rate of all layers\"\n",
    "    \"\"\"\n",
    "\n",
    "    global learning_rate\n",
    "    global learning_rate_DN\n",
    "\n",
    "\n",
    "    for param_group in optimizer_EB1.param_groups:\n",
    "        param_group['lr'] = learning_rate\n",
    "    for param_group in optimizer_EB2.param_groups:\n",
    "        param_group['lr'] = learning_rate\n",
    "    for param_group in optimizer_EB3.param_groups:\n",
    "        param_group['lr'] = learning_rate\n",
    "    for param_group in optimizer_EB4.param_groups:\n",
    "        param_group['lr'] = learning_rate\n",
    "    for param_group in optimizer_FB.param_groups:\n",
    "        param_group['lr'] = learning_rate\n",
    "    for param_group in optimizer_DN3.param_groups:\n",
    "        param_group['lr'] = learning_rate_DN*-1\n",
    "    for param_group in optimizer_DN1.param_groups:\n",
    "        param_group['lr'] = learning_rate_DN*-1\n",
    "    for param_group in optimizer_DN2.param_groups:\n",
    "        param_group['lr'] = learning_rate_DN*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "### Calculate loss of VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf1tE0O7V274"
   },
   "outputs": [],
   "source": [
    "def calc_loss(true_labels : np.ndarray, predictions : np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the loss from the VAD output, by comparing it to the true labels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_labels : numpy array\n",
    "        The true labels of the data\n",
    "    predictions : numpy array\n",
    "        The predictions made by the model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        The loss calculated by the loss function, BCELoss\n",
    "    \"\"\"\n",
    "    labels_two_channels = np.zeros((2,len(true_labels[0,:])))\n",
    "    labels_two_channels = torch.from_numpy(labels_two_channels).to(device)\n",
    "    index_min = min(len(true_labels[0,:]),len(predictions[0,0,:]))\n",
    "    labels_two_channels[0,:] = true_labels\n",
    "    labels_two_channels[1,:] = 1-true_labels\n",
    "    labels_two_channels = labels_two_channels[:,0:index_min]\n",
    "\n",
    "    loss = loss_primary(predictions[0,:,0:index_min].T.float(),labels_two_channels[:,:].T.float())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "### Calculate loss of DN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVR3bqlRWIqg"
   },
   "outputs": [],
   "source": [
    "def calc_loss_noisetypes(noise_true : np.ndarray, noise_predicted : np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the loss from the discriminative network, by comparing it to the true noise types\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    noise_true : numpy array\n",
    "        The true noise types of the data\n",
    "    noise_predicted : numpy array\n",
    "        The predictions made by the model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        The loss calculated by the loss function, CrossEntropyLoss\n",
    "    \"\"\"\n",
    "    noise_true = torch.reshape(noise_true, (-1,))\n",
    "    loss = loss_secondary(noise_predicted.T, noise_true.long())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yESDa2ncWYWE"
   },
   "outputs": [],
   "source": [
    "def back_propagation_full(loss : float, t : int) -> None:\n",
    "    \"\"\"\n",
    "    Performs the backward step to calculate gradients, then updates the parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss : float\n",
    "        The loss calculated by the loss function\n",
    "    t : int\n",
    "        The number of forward steps that have been taken (unused for now)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer_EB1.zero_grad()\n",
    "    optimizer_EB2.zero_grad()\n",
    "    optimizer_EB3.zero_grad()\n",
    "    optimizer_EB4.zero_grad()\n",
    "    optimizer_FB.zero_grad()\n",
    "    optimizer_DN1.zero_grad()\n",
    "    optimizer_DN2.zero_grad()\n",
    "    optimizer_DN3.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer_EB1.step()\n",
    "    optimizer_EB2.step()\n",
    "    optimizer_EB3.step()\n",
    "    optimizer_EB4.step()\n",
    "\n",
    "    optimizer_FB.step()\n",
    "\n",
    "    optimizer_DN1.step()\n",
    "    optimizer_DN2.step()\n",
    "    optimizer_DN3.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "### After batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkkfiofKWp4p"
   },
   "outputs": [],
   "source": [
    "def after_batches(batch_size: int, accumulated_accuracy: float, loss_acc: float, \\\n",
    "                  batch: int, loss: float, loss_AN: float, X: torch.Tensor, accuracy: float, size: int, loss_L2: float) -> None:\n",
    "    \"\"\"\n",
    "    Prints various information in the console after each backward step\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        The number of forward steps per backward step\n",
    "    accumulated_accuracy : float\n",
    "        The accuracy of the model, it is accumulated over the batch size\n",
    "    loss_acc : float\n",
    "        The accumulated loss over the batch size\n",
    "    batch : int\n",
    "        The number of forward steps that have been taken\n",
    "    loss : float\n",
    "        The loss calculated by the loss function\n",
    "    X : Tensor\n",
    "        The input data\n",
    "    accuracy : float\n",
    "        The accuracy of the model over the batch (unused for now)\n",
    "    size : int\n",
    "        The total number of files in the dataset\n",
    "    loss_L2 : float\n",
    "        The L2 loss calculated by the loss function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    accumulated_accuracy /= batch_size\n",
    "    loss, current = loss.item(), batch * len(X)\n",
    "    loss_acc /= batch_size\n",
    "\n",
    "    print(f\"Current file/Number of files: [{current+1:>5d}/{size:>5d}]\")\n",
    "    print(f\"loss VAD: {loss_acc:>7f}\")\n",
    "    print(f\"Loss DN: {loss_AN}\")\n",
    "    print(f\"Loss L: {loss_L2}\")\n",
    "    print(f\"Accuracy of batch: {accumulated_accuracy*100:>4f}\")\n",
    "    print(f\"Learning rate: {learning_rate:>4f}\")\n",
    "    print(f\"Learning rate DN: {learning_rate_DN:>4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "### Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MABGp5t3Wy8x"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(pred: torch.Tensor, y: torch.Tensor) -> tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Finds the predicted labels by comparing the speech and non-speech channels, then calculates the accuracy. Returns both\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : Tensor\n",
    "        The predicted output of the model\n",
    "    y : Tensor\n",
    "        The true labels of the data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : float\n",
    "        The accuracy of the model\n",
    "    labs : numpy array\n",
    "        The predicted labels\n",
    "    \"\"\"\n",
    "    labs = (pred[:,0,0]>pred[:,1,0]).to('cpu').detach().numpy()\n",
    "    y = y[:,0:len(labs)]\n",
    "    labs = labs[0:len(y[0,:])]\n",
    "    accuracy = 1-sum((abs(labs-y[0,:].to('cpu').detach().numpy())))/len(y[0,:].to('cpu').detach().numpy())\n",
    "    return accuracy, labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "### Make plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJyioEbMsoQ-"
   },
   "outputs": [],
   "source": [
    "def make_plots(preds : np.ndarray, X : torch.Tensor, y : torch.Tensor, labs : np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Plots the raw waveform, scores of speech and non-speech, VAD predictions and true VAD labels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : numpy array\n",
    "        The predictions made by the model\n",
    "    X : numpy array\n",
    "        The input data\n",
    "    y : numpy array\n",
    "        The true labels of the data\n",
    "    labs : numpy array\n",
    "        The predicted labels\n",
    "    \"\"\"\n",
    "    y = y[0:len(labs)]\n",
    "    plt.plot(X[0:len(preds[0,:])*160]/max(X)*0.5, label=\"Amplitude\")\n",
    "    samples_p_label = np.ones((160,))\n",
    "    # plt.plot(np.kron(preds[0,:], samples_p_label)-0.5,'g')\n",
    "    # plt.plot(np.kron(preds[1,:], samples_p_label)-0.5,'r')\n",
    "    plt.plot((np.kron(y, samples_p_label))-0.5,'g', label=\"True labels\")\n",
    "    plt.plot((np.kron(labs, samples_p_label))*0.5+0.6, 'r',label=\"Predicted labels\")\n",
    "    plt.plot((np.kron(abs(labs-y), samples_p_label))*0.5-1.4, label=\"False predicted points\")\n",
    "\n",
    "    new_acc = {1-sum((labs-y!=0))/len(y)}\n",
    "    plt.ylim([-1.1,1.3])\n",
    "    plt.title(f\"SNR level: {snr} bB - acc: {new_acc}\")\n",
    "\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    plt.pause(0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "### Plot ROC and calculate AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0irceuRsy-WQ"
   },
   "outputs": [],
   "source": [
    "def calculate_AUC_and_plot_ROC() -> None:\n",
    "\n",
    "  \"\"\"\n",
    "  Plot roc curve and calculate auc score.\n",
    "  \"\"\"\n",
    "  y_TP_rate = []\n",
    "  x_FP_rate = []\n",
    "\n",
    "  length = len(training_results_AUC[\"TP\"])\n",
    "\n",
    "  for i in range(length):\n",
    "    y_rate = (training_results_AUC[\"TP\"][i])/(training_results_AUC[\"TP\"][i] + training_results_AUC[\"FN\"][i])\n",
    "    x_rate = (training_results_AUC[\"FP\"][i])/(training_results_AUC[\"FP\"][i] + training_results_AUC[\"TN\"][i])\n",
    "\n",
    "    y_TP_rate.append(y_rate)\n",
    "    x_FP_rate.append(x_rate)\n",
    "\n",
    "  auc_score = auc(x_FP_rate, y_TP_rate)\n",
    "\n",
    "  plt.plot(x_FP_rate, y_TP_rate, marker='o', linestyle='-')  # Plot with markers and line\n",
    "  plt.fill_between(x_FP_rate, y_TP_rate, alpha=0.2, color='skyblue') # Fill area under the curve\n",
    "\n",
    "\n",
    "  # Add AUC score text\n",
    "  plt.text(0.6, 0.2, f\"AUC = {auc_score:.4f}\", fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "  plt.xlabel(\"False Positive Rate\")\n",
    "  plt.ylabel(\"True Positive Rate\")\n",
    "  plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-weight: bold;\n",
    "        font-style: italic;\n",
    "        font-family: Tw Cen MT;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "### Count number of element in confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c5qD2aXnpSt"
   },
   "outputs": [],
   "source": [
    "def calc_pos_neg(labels : np.ndarray, predictions : np.ndarray) -> tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Calculates the number of true positive, false positives, true negatives and false negatives in the last forward step\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : numpy array\n",
    "        The true labels of the data\n",
    "    predictions : numpy array\n",
    "        The predictions made by the model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : int\n",
    "        The number of true positives, the model predicted speech and it was speech\n",
    "    FP : int\n",
    "        The number of false positives, the model predicted speech and it was not speech\n",
    "    TN : int\n",
    "        The number of true negatives, the model predicted non-speech and it was non-speech\n",
    "    FN : int\n",
    "        The number of false negatives, the model predicted non-speech and it was speech\n",
    "    \"\"\"\n",
    "    labels = labels[0:len(predictions)]\n",
    "    predictions_inv = (predictions-1)*-1;\n",
    "    labels_inv = (labels-1)*-1;\n",
    "    TP = sum(predictions[labels[0:len(predictions)]==1])\n",
    "    FP = sum(predictions[labels[0:len(predictions)]==0])\n",
    "    FN = sum(predictions_inv[labels[0:len(predictions)]==1])\n",
    "    TN = sum(predictions_inv[labels_inv[0:len(predictions)]==1])\n",
    "    return TP, FP, TN, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqV-_Uk54-My"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-style: italic;\n",
    "        font-family: Roboto;\n",
    "    }\n",
    "</style>\n",
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1x19Uz6SW6Ep"
   },
   "outputs": [],
   "source": [
    "def train_loop(train_data_loader : DataLoader, t : int) -> None:\n",
    "    \"\"\"Variable initialisations\"\"\"\n",
    "    size = len(train_data_loader.dataset)\n",
    "    total_acc = 0\n",
    "    total_loss_DB = 0\n",
    "    total_loss_AN = 0\n",
    "    loss_acc = 0\n",
    "    loss_AN_acc = 0\n",
    "    accumulated_accuracy = 0\n",
    "    comb_loss = 0\n",
    "    last_batch = -1\n",
    "    concats = concatenates # The number of files to concatenate\n",
    "\n",
    "    global noises_buffer\n",
    "\n",
    "    if(len(noises_buffer) == 0):\n",
    "      noise_1, _ = librosa\\\n",
    "      .load(f\"/content/{folder_name}/noises_{snr_noise_str}/babble_3s_{snr_noise}.wav\", sr=16000)\n",
    "      noise_2, _ = librosa\\\n",
    "      .load(f\"/content/{folder_name}/noises_{snr_noise_str}/factory_3s_{snr_noise}.wav\", sr=16000)\n",
    "\n",
    "      noises_buffer = [noise_1, noise_2]\n",
    "\n",
    "\n",
    "    \"\"\" Initialises empty tensors for the data to be concatenated\"\"\"\n",
    "    concat_X = torch.empty((1,1,1,0), device = device)\n",
    "    concat_y = torch.empty((1,0), device = device)\n",
    "    concat_noise = torch.empty((1,0), device = device)\n",
    "    bcounter = 0\n",
    "\n",
    "    for batch, (X, y, noise_true, SNR) in enumerate(train_data_loader):\n",
    "\n",
    "        y = y.to(device)\n",
    "        X = X.to(device)\n",
    "\n",
    "\n",
    "        \"\"\"Add randomly 0.5s - 2.0s noises to end of tensor \"\"\"\n",
    "\n",
    "        number_of_frame = random.randint(50, 200)\n",
    "        index_of_noise = random.randint(0,1)\n",
    "\n",
    "        raw_buffer = noises_buffer[index_of_noise][0:number_of_frame*160]\n",
    "\n",
    "        buffer = np.zeros((1,1,len(raw_buffer)))\n",
    "\n",
    "        buffer[0,0,:] = np.hstack(raw_buffer)\n",
    "\n",
    "        buffer_label = np.zeros(number_of_frame)\n",
    "\n",
    "        buffer = torch.from_numpy(buffer).to(device)\n",
    "        buffer = torch.unsqueeze(buffer, 0)\n",
    "\n",
    "        buffer_label = torch.from_numpy(buffer_label).to(device)\n",
    "        buffer_label = torch.unsqueeze(buffer_label, 0)\n",
    "\n",
    "\n",
    "        buffer_noise = np.zeros(np.shape(buffer_label)) + index_of_noise\n",
    "        buffer_noise = torch.from_numpy(buffer_noise).to(device)\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"Creates a tensor of similar size to y\n",
    "        containing the true labels for noise type\"\"\"\n",
    "        noises = [\"N1\", \"N2\", \"CL\"]\n",
    "        noise_index = noises.index(''.join(noise_true))\n",
    "\n",
    "        noise_vector_ini = np.zeros(np.shape(y)) + noise_index\n",
    "        noise_vector_ini = torch.from_numpy(noise_vector_ini).to(device)\n",
    "\n",
    "\n",
    "        \"\"\"Concatenate audio, VAD labels and noise labels\"\"\"\n",
    "        if len(X[0,0,0,:]) != 0:\n",
    "            concat_X = torch.cat((concat_X, X),3)\n",
    "            concat_y = torch.cat((concat_y, y),1)\n",
    "            concat_noise = torch.cat((concat_noise, noise_vector_ini),1)\n",
    "\n",
    "        \"\"\"The main training loop.\n",
    "         Runs after sufficient files are concatenated\"\"\"\n",
    "        if batch >= last_batch + concats:\n",
    "\n",
    "            bcounter +=1\n",
    "            last_batch = batch # Counter variable\n",
    "            \"\"\"Stores the data in original variable names\n",
    "             and resets the tensors containing the concatenated files\"\"\"\n",
    "            X = concat_X\n",
    "            y = concat_y\n",
    "            noise = concat_noise\n",
    "\n",
    "            concat_X = torch.empty((1,1,1,0), device = device)\n",
    "            concat_y = torch.empty((1,0), device = device)\n",
    "            concat_noise = torch.empty((1,0), device = device)\n",
    "\n",
    "            concat_X = torch.cat((concat_X, buffer),3)\n",
    "            concat_y = torch.cat((concat_y, buffer_label),1)\n",
    "            concat_noise = torch.cat((concat_noise, buffer_noise),1)\n",
    "\n",
    "\n",
    "\n",
    "            \"\"\"Forward step\"\"\"\n",
    "\n",
    "            pred_DB, pred_AN = VAD(X[0,:,:,:].float(), training=1)\n",
    "\n",
    "            loss_DB = calc_loss(y,pred_DB)\n",
    "            loss_AN = calc_loss_noisetypes(\\\n",
    "                                           noise[:,0:len(pred_AN[0,0,:])], \\\n",
    "                                           pred_AN[0,:,0:len(noise[0,:])])\n",
    "\n",
    "            l2_penalty = l2_weight * sum([(p**2).sum() for p in VAD.parameters() if p.requires_grad])\n",
    "\n",
    "            \"\"\"Variables storing the accumulated losses\"\"\"\n",
    "            total_loss_DB += loss_DB.item() # Accumulated loss over full training epoch\n",
    "            loss_acc += loss_DB.item()      # Accumulated loss between backward steps\n",
    "            loss_AN_acc += loss_AN.item()\n",
    "\n",
    "            comb_loss += (1*loss_DB - AN_weight*loss_AN + l2_penalty)  # Accumulated combined loss of VAD and noise - including the computational graph\n",
    "\n",
    "            \"\"\"Calculates the predicted VAD labels and returns the accuracy\"\"\"\n",
    "            accuracy, _ = calc_accuracy(pred_DB.T, y)\n",
    "            labs = (pred_DB[0,0,:]>pred_DB[0,1,:]).to('cpu').detach().numpy()\n",
    "            accumulated_accuracy += accuracy\n",
    "            total_acc += accuracy\n",
    "\n",
    "            \"\"\"Backward step\"\"\"\n",
    "            if bcounter == training_batch_size:\n",
    "                bcounter = 0\n",
    "                comb_loss = comb_loss/training_batch_size # Finding the mean of the loss\n",
    "                back_propagation_full(comb_loss, t) # Performs the backpropagation and optimisation step\n",
    "\n",
    "                after_batches(training_batch_size, accumulated_accuracy, loss_acc, batch, loss_DB, loss_AN_acc, X, accuracy, size, l2_penalty) # Prints information about the latest forward step to the console. Comment to keep the console clean\n",
    "\n",
    "                \"\"\"Resets variables\"\"\"\n",
    "                loss_acc = 0\n",
    "                loss_AN_acc = 0\n",
    "                accumulated_accuracy = 0\n",
    "                comb_loss = 0\n",
    "\n",
    "\n",
    "    training_results_big[\"training\"].append(total_acc/size)\n",
    "    training_results_big[\"learning_rate\"].append(learning_rate)\n",
    "    training_results_big[\"epochs\"].append(t)\n",
    "    training_results_big[\"loss_DB\"].append(total_loss_DB/size)\n",
    "    training_results_big[\"loss_AN\"].append(total_loss_AN/size)\n",
    "    save_model_path = f\"model_epoch_{t}_{snr_str}_WODB.pth\"\n",
    "    torch.save(VAD.state_dict(), save_model_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7GKzJUIo_Ro"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-style: italic;\n",
    "        font-family: Roboto;\n",
    "    }\n",
    "</style>\n",
    "## Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLaRnrVWrDGp"
   },
   "outputs": [],
   "source": [
    "def test_loop(test_data_loader : DataLoader, t : int) -> None:\n",
    "    \"\"\"Variable initialisations\"\"\"\n",
    "    last_batch = -1\n",
    "    concats = concatenates # The number of files to concatenate\n",
    "\n",
    "    global noises_buffer\n",
    "\n",
    "    if(len(noises_buffer) == 0):\n",
    "      noise_1, _ = librosa\\\n",
    "      .load(f\"/content/{folder_name}/noises_{snr_noise_str}/babble_3s_{snr_noise}.wav\", sr=16000)\n",
    "      noise_2, _ = librosa\\\n",
    "      .load(f\"/content/{folder_name}/noises_{snr_noise_str}/factory_3s_{snr_noise}.wav\", sr=16000)\n",
    "\n",
    "      noises_buffer = [noise_1, noise_2]\n",
    "\n",
    "\n",
    "    \"\"\" Initialises empty tensors for the data to be concatenated\"\"\"\n",
    "    concat_X = torch.empty((1,1,1,0), device = device)\n",
    "    concat_y = torch.empty((1,0), device = device)\n",
    "\n",
    "    for batch, (X, y, noise_true, SNR) in enumerate(test_data_loader):\n",
    "\n",
    "        if(batch > 30): return;\n",
    "\n",
    "        y = y.to(device)\n",
    "        X = X.to(device)\n",
    "\n",
    "\n",
    "        \"\"\"Add randomly 0.5s - 2.0s noises to end of tensor \"\"\"\n",
    "\n",
    "        number_of_frame = random.randint(50, 200)\n",
    "        index_of_noise = random.randint(0,1)\n",
    "\n",
    "        raw_buffer = noises_buffer[index_of_noise][0:number_of_frame*160]\n",
    "\n",
    "        buffer = np.zeros((1,1,len(raw_buffer)))\n",
    "\n",
    "        buffer[0,0,:] = np.hstack(raw_buffer)\n",
    "\n",
    "        buffer_label = np.zeros(number_of_frame)\n",
    "\n",
    "        buffer = torch.from_numpy(buffer).to(device)\n",
    "        buffer = torch.unsqueeze(buffer, 0)\n",
    "\n",
    "        buffer_label = torch.from_numpy(buffer_label).to(device)\n",
    "        buffer_label = torch.unsqueeze(buffer_label, 0)\n",
    "\n",
    "\n",
    "        \"\"\"Concatenate audio, VAD labels and noise labels\"\"\"\n",
    "        if len(X[0,0,0,:]) != 0:\n",
    "            concat_X = torch.cat((concat_X, X),3)\n",
    "            concat_y = torch.cat((concat_y, y),1)\n",
    "\n",
    "        \"\"\"The main training loop.\n",
    "         Runs after sufficient files are concatenated\"\"\"\n",
    "        if batch >= last_batch + concats:\n",
    "\n",
    "            last_batch = batch # Counter variable\n",
    "            \"\"\"Stores the data in original variable names\n",
    "             and resets the tensors containing the concatenated files\"\"\"\n",
    "            X = concat_X\n",
    "            y = concat_y\n",
    "\n",
    "            concat_X = torch.empty((1,1,1,0), device = device)\n",
    "            concat_y = torch.empty((1,0), device = device)\n",
    "\n",
    "            concat_X = torch.cat((concat_X, buffer),3)\n",
    "            concat_y = torch.cat((concat_y, buffer_label),1)\n",
    "\n",
    "\n",
    "\n",
    "            \"\"\"Forward step\"\"\"\n",
    "            start_time = time.time()\n",
    "            pred_DB, pred_AN = VAD(X[0,:,:,:].float())\n",
    "            end_time = time.time()\n",
    "\n",
    "            \"\"\"Time to excute model\"\"\"\n",
    "            execution_time = (end_time - start_time) * 1000\n",
    "\n",
    "\n",
    "            \"\"\"Calculates the predicted VAD labels and returns the accuracy\"\"\"\n",
    "            accuracy, _ = calc_accuracy(pred_DB.T, y)\n",
    "            labs = (pred_DB[0,0,:]>pred_DB[0,1,:]).to('cpu').detach().numpy()\n",
    "\n",
    "            print(f\"Accuracy : {accuracy}\")\n",
    "\n",
    "            npX = X[0,0,0,:].to('cpu').detach().numpy()\n",
    "            npy = y[0,:].to('cpu').detach().numpy()\n",
    "            preds_np = pred_DB[0,:,:].to('cpu').detach().numpy()\n",
    "\n",
    "            ipd.display(ipd.Audio(npX, rate=16000))\n",
    "\n",
    "            print('Make plot')\n",
    "            make_plots(preds_np, npX, npy, labs)\n",
    "\n",
    "            \"\"\"ROC plot\"\"\"\n",
    "            ROC_samples = 51\n",
    "            TP_acc = np.zeros((ROC_samples,))\n",
    "            FP_acc = np.zeros((ROC_samples,))\n",
    "            TN_acc = np.zeros((ROC_samples,))\n",
    "            FN_acc = np.zeros((ROC_samples,))\n",
    "            accumulated_acc = np.zeros((ROC_samples,))\n",
    "\n",
    "            for index, (threshold) in enumerate(np.linspace(0,1,ROC_samples)):\n",
    "\n",
    "                labs = (pred_DB[0,0,:]>threshold).to('cpu').detach().numpy()\n",
    "                npy=y[0,:].to('cpu').detach().numpy()\n",
    "                npy = npy[0:len(labs)]\n",
    "\n",
    "                \"\"\" Calculates  the number of true positives, false positive etc.\"\"\"\n",
    "                TP, FP, TN, FN = calc_pos_neg(npy, labs)\n",
    "\n",
    "                \"\"\"Each index corresponds to a threshold value\"\"\"\n",
    "                TP_acc[index] += TP\n",
    "                FP_acc[index] += FP\n",
    "                TN_acc[index] += TN\n",
    "                FN_acc[index] += FN\n",
    "\n",
    "\n",
    "            total_samples = FP_acc + TN_acc + TP_acc + FN_acc\n",
    "\n",
    "            training_results_AUC[f\"TP\"] = (TP_acc/total_samples)\n",
    "            training_results_AUC[f\"FP\"] = (FP_acc/total_samples)\n",
    "            training_results_AUC[f\"TN\"] = (TN_acc/total_samples)\n",
    "            training_results_AUC[f\"FN\"] = (FN_acc/total_samples)\n",
    "\n",
    "\n",
    "            print(f\"Time to execute model: {execution_time}\")\n",
    "            calculate_AUC_and_plot_ROC()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usg7h16_nme1"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-style: italic;\n",
    "        font-family: Roboto;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1.25em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "# Evaluation of the Model\n",
    "Running functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BXb8bqUhL4k"
   },
   "outputs": [],
   "source": [
    "def run_train(t : int) -> None:\n",
    "\n",
    "    \"\"\"Training process\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : int\n",
    "        The number of epochs that have been run\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    global learning_rate, learning_rate_DN, LR_factor\n",
    "\n",
    "    print(f\"Epoch {t+1}\\n--------------TRAIN-----------------\")\n",
    "    VAD.train()\n",
    "    dataset_train = TIMIT_train()\n",
    "    train_data_loader = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "    update_learning_rates()\n",
    "    train_loop(train_data_loader, t) # The main training loop\n",
    "\n",
    "    \"\"\"Updates the learning rate after each epoch\"\"\"\n",
    "    learning_rate *= LR_factor\n",
    "    learning_rate_DN *= LR_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loLeHjVwt5ZZ"
   },
   "outputs": [],
   "source": [
    "def run_test(t : int) -> None:\n",
    "\n",
    "    \"\"\"Testing process\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : int\n",
    "        The number of epochs that have been run\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Epoch {t+1}\\n--------------TEST-----------------\")\n",
    "    dataset_test = TIMIT_test()\n",
    "    test_data_loader = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "    test_loop(test_data_loader, t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUfRLyQv_tjc"
   },
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        color: #FBA2D0;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h2 {\n",
    "        color: #62825D;\n",
    "        font-weight: bolder;\n",
    "        font-family: Aptos;\n",
    "    }\n",
    "    h3 {\n",
    "        color: #FA9189;\n",
    "        font-style: italic;\n",
    "        font-family: Roboto;\n",
    "    }\n",
    "    * {\n",
    "        font-family: Tw Cen MT;\n",
    "        font-size: 1.25em;\n",
    "        color: #465E5B;\n",
    "    }\n",
    "</style>\n",
    "## Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1169850,
     "status": "ok",
     "timestamp": 1732890461576,
     "user": {
      "displayName": "Phú Quốc Đinh",
      "userId": "18201426563846769527"
     },
     "user_tz": -420
    },
    "id": "b-76UKbtifTj",
    "outputId": "851bb847-a2ab-4622-ceb7-85d9aeeec73b"
   },
   "outputs": [],
   "source": [
    "pretrained = True\n",
    "continue_train = False\n",
    "\n",
    "path_to_model = \"\"\n",
    "\n",
    "if(pretrained):\n",
    "  load_model(path_to_model)\n",
    "  run_test(19)\n",
    "\n",
    "else:\n",
    "  for t in range(training_epochs):\n",
    "      run_train(t)\n",
    "      run_test(t)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
